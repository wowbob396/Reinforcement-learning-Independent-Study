{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Reinforcement Learning and Q-Learning\n",
    "\n",
    "In the first week of my independent study, I looked at several brief resources that covered a wide variety of reinforcement learning topics from dynamic programming to SARSA, Markov Decision Processes, and Monte Carlo trees.\n",
    "\n",
    "What I wanted to focus on first, though, is Q-Learning.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Reinforcement Learning?\n",
    "\n",
    "Reinforcement learning is a subset of machine learning that is concerned with how agents should behave (take actions) in a given environment.\n",
    "\n",
    "\n",
    "A reinforcement learning algorithm has the following components:\n",
    "\n",
    "- S -> set of states\n",
    "- A -> set of actions\n",
    "- Pr(s'|a,s) -> Transition probability, the probability of transitioning to a new state given a current state and action in the current state\n",
    "- $\\alpha$ -> starting state distribution\n",
    "- $\\gamma$ -> discount factor\n",
    "- r(s,a) -> reward given a state and an action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Q-Learning?\n",
    "\n",
    "Q-Learning is a reinforcement learning technique that allows an agent to learn a policy for how to behave in a given environment.\n",
    "\n",
    "Q stands for the __*quality*__ of an action in a given state.\n",
    "\n",
    "\n",
    "More specifically, the aim for this approach is to obtain a function, $Q(s,a)$ that predictions the best action a in state s to maximize the cumulative \"reward\" value\n",
    "\n",
    "This function is iteratively updated via the Bellman equation, which is as follows:\n",
    "\n",
    "$Q(s,a) = r + \\gamma max_{a'}Q(s',a')$\n",
    "\n",
    "This first term, r, is the immediate reward, and the second term is the future reward.\n",
    "\n",
    "In a relatively simple environment, the Q-Learning Algorithm can be represented as a matrix where the rows represent the actions, and the columns represent the states.  The cells themselves are the rewards for the state-action pair\n",
    "\n",
    "*A sample representation of a Q-Matrix*\n",
    "\n",
    "$\n",
    "Q =\n",
    "  \\begin{bmatrix}\n",
    "    1 & 2 & 3 \\\\\n",
    "    3 & 1 & 5  \\\\\n",
    "    3 & 3 & -5 \n",
    "  \\end{bmatrix}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing our first Q-Learning algorithm\n",
    "\n",
    "*[The guide I am following can be found here](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/Q%20Learning%20with%20FrozenLake.ipynb)*\n",
    "\n",
    "Using the OpenAI Gym library, we can create reinforcement learning algorithms using virtual environments that allows us to control agents within the environment.\n",
    "\n",
    "Before we begin, we must import the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enables intellisense (press TAB after the .)\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Q-table\n",
    "\n",
    "Referring back to the previous cells, the Q-table's representation is state x action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "q_table = torch.zeros((state_size,action_size))\n",
    "\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 15000 \n",
    "learning_rate = 0.8\n",
    "max_steps = 99\n",
    "gamma = 0.95\n",
    "\n",
    "epsilon = 1.0\n",
    "max_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.005\n",
    "\n",
    "print(q_table)\n",
    "print(q_table[state,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    # Reset the environment!!\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Choose an action a in the current world state (s)\n",
    "        ## We randomize a number\n",
    "        exp_exp_tradeoff = random.uniform(0,1)\n",
    "        \n",
    "        ## if this number is greater than epsilon, we take that action which corresponds to the biggest Q-value in a given state (Exploitation)\n",
    "        if exp_exp_tradeoff > epsilon:\n",
    "            action = torch.argmax(q_table[state,:]).item()\n",
    "        # Else doing a random choice\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        # take an action (a), and observe the new state (s') and reward (r)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        q_table[state,action] = q_table[state,action] + learning_rate * (reward + gamma * torch.max(q_table[new_state,:] - q_table[state,action]))\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        state = new_state\n",
    "        \n",
    "        # if done equals true, we died\n",
    "        if done == True:\n",
    "            break\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*episode)\n",
    "        \n",
    "print (\"Score over time: \" +  str(sum(rewards)/total_episodes))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
