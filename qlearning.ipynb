{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Reinforcement Learning and Q-Learning\n",
    "\n",
    "In the first week of my independent study, I looked at several brief resources that covered a wide variety of reinforcement learning topics from dynamic programming to SARSA, Markov Decision Processes, and Monte Carlo trees.\n",
    "\n",
    "What I wanted to focus on first, though, is Q-Learning.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Reinforcement Learning?\n",
    "\n",
    "Reinforcement learning is a subset of machine learning that is concerned with how agents should behave (take actions) in a given environment.\n",
    "\n",
    "\n",
    "A reinforcement learning algorithm has the following components:\n",
    "\n",
    "- S -> set of states\n",
    "- A -> set of actions\n",
    "- Pr(s'|a,s) -> Transition probability, the probability of transitioning to a new state given a current state and action in the current state\n",
    "- $\\alpha$ -> starting state distribution\n",
    "- $\\gamma$ -> discount factor\n",
    "- r(s,a) -> reward given a state and an action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Q-Learning?\n",
    "\n",
    "Q-Learning is a reinforcement learning technique that allows an agent to learn a policy for how to behave in a given environment.\n",
    "\n",
    "Q stands for the __*quality*__ of an action in a given state.\n",
    "\n",
    "\n",
    "More specifically, the aim for this approach is to obtain a function, $Q(s,a)$ that predictions the best action a in state s to maximize the cumulative \"reward\" value\n",
    "\n",
    "This function is iteratively updated via the Bellman equation, which is as follows:\n",
    "\n",
    "$Q(s,a) = r + \\gamma max_{a'}Q(s',a')$\n",
    "\n",
    "This first term, r, is the immediate reward, and the second term is the future reward.\n",
    "\n",
    "In a relatively simple environment, the Q-Learning Algorithm can be represented as a matrix where the rows represent the actions, and the columns represent the states.  The cells themselves are the rewards for the state-action pair\n",
    "\n",
    "*A sample representation of a Q-Matrix*\n",
    "\n",
    "$\n",
    "Q =\n",
    "  \\begin{bmatrix}\n",
    "    1 & 2 & 3 \\\\\n",
    "    3 & 1 & 5  \\\\\n",
    "    3 & 3 & -5 \n",
    "  \\end{bmatrix}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
