{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an AI to Play OpenAI's CartPole Simulation\n",
    "\n",
    "### Deep Q-Network (DQN)\n",
    "\n",
    "Due the the continous nature of this environment, approximating all of the possible action,states is inefficient, and uses up a substantial amount of resources for a fairly simple environment.  Instead, we will be using a deep Q-network.  A DQN works by approximating the optimal value function through the use of neural networks, as opposed to generating a Q-table.\n",
    "\n",
    "In this approach, a simple neural network will be used to generate the optimal value function for the CartPole scenario!\n",
    "\n",
    "The tutorial I will be following can be found [here.](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enables intellisense (press TAB after the .)\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the environment, and the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DQN will be utilizing replay memory, and this replay memory will be randomly sampled to help aid in the agent's decision making.  Since the agent samples from the replay memory randomly, the transitions that build up the batch will now be decorrelated.\n",
    "\n",
    "There are to classes involved with this, first, the `Transition` class, and the `ReplayMemory` class.\n",
    "\n",
    "- `Transition`: A named tuple that represents a single transition in an environment.\n",
    "- `ReplayMemory`: A cyclie buffer of bounded size that maintains recent transitions.  It contains a `.sample()` method, to randomly retrieve a transition batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',('state','action','next_state','reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
